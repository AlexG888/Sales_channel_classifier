{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSpli\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dim_red(model, features, labels, n_components=2):\n",
    "\n",
    "    # Creation of the model\n",
    "    if (model == 'PCA'):\n",
    "        mod = PCA(n_components=n_components)\n",
    "        title = \"PCA decomposition\"  # for the plot\n",
    "\n",
    "    elif (model == 'TSNE'):\n",
    "        mod = TSNE(n_components=2)\n",
    "        title = \"t-SNE decomposition\"\n",
    "\n",
    "    else:\n",
    "        return \"Error\"\n",
    "\n",
    "    # Fit and transform the features\n",
    "    principal_components = mod.fit_transform(features)\n",
    "\n",
    "    # Put them into a dataframe\n",
    "    df_features = pd.DataFrame(data=principal_components,\n",
    "                     columns=['PC1', 'PC2'])\n",
    "\n",
    "    # Now we have to paste each row's label and its meaning\n",
    "    # Convert labels array to df\n",
    "    df_labels = pd.DataFrame(data=labels,\n",
    "                             columns=['label'])\n",
    "\n",
    "    df_full = pd.concat([df_features, df_labels], axis=1)\n",
    "    df_full['label'] = df_full['label'].astype(str)\n",
    "\n",
    "    # Get labels name\n",
    "    category_names = {\n",
    "    \"0\":'Hospital',\n",
    "    '1':'COVID',\n",
    "    '2':'Retail',\n",
    "    '3':'Reimbursement',\n",
    "    '4':'Program'\n",
    "}\n",
    "\n",
    "    # And map labels\n",
    "    df_full['label_name'] = df_full['label']\n",
    "    df_full = df_full.replace({'label_name':category_names})\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.scatterplot(x='PC1',\n",
    "                    y='PC2',\n",
    "                    hue=\"label_name\",\n",
    "                    data=df_full,\n",
    "                    palette=[\"red\", \"yellow\", \"royalblue\", \"greenyellow\", \"lightseagreen\"],\n",
    "                    alpha=.7).set_title(title)\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem()\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_Teders = dataiku.Dataset(\"IM_Teders\")\n",
    "im_Teders_df = im_Teders.get_dataframe()\n",
    "im_Teders_df_copy = im_Teders_df\n",
    "\n",
    "im_Teders_df['Prod_Package'] = im_Teders_df['Prod_RefStd_FormName'].str.extract(r'(№\\d+)')\n",
    "im_Teders_df['Prod_Package'] = im_Teders_df['Prod_Package'].str.replace('№','')\n",
    "im_Teders_df['Prod_Package'][(im_Teders_df['Prod_Package'].isna())|\n",
    "                  (im_Teders_df['Prod_Package'] == 'не определено')] = 1\n",
    "im_Teders_df['Prod_Package'] = im_Teders_df['Prod_Package'].astype(float)\n",
    "im_Teders_df['LotSpec_Doses'] = im_Teders_df['LotSpec_Num']\n",
    "im_Teders_df['Prod_Doses'] = im_Teders_df['Prod_NumPack'] * im_Teders_df['Prod_Package']\n",
    "\n",
    "\n",
    "im_Teders_df.loc[im_Teders_df['LotSpec_MNN_InnE'] == 'Rivaroxaban', 'LotSpec_DOT'] = im_Teders_df['LotSpec_Doses']\n",
    "im_Teders_df.loc[im_Teders_df['LotSpec_MNN_InnE'].isin(['Warfarin', 'Dabigatran etexilate', 'Apixaban']), 'LotSpec_DOT'] = im_Teders_df['LotSpec_Doses']/2\n",
    "\n",
    "im_Teders_df.loc[im_Teders_df['LotSpec_MNN_InnE'] == 'Rivaroxaban', 'Prod_DOT'] = im_Teders_df['Prod_Doses']\n",
    "im_Teders_df.loc[im_Teders_df['LotSpec_MNN_InnE'].isin(['Warfarin', 'Dabigatran etexilate', 'Apixaban']), 'Prod_DOT'] = im_Teders_df['Prod_Doses']/2\n",
    "\n",
    "im_Teders_df = im_Teders_df[im_Teders_df['Contr_SignDate']  >= '2019-12-01' ]\n",
    "im_Teders_df = im_Teders_df[im_Teders_df['LotSpec_MNN_InnE'].isin(['Warfarin', 'Dabigatran etexilate',\n",
    "                                                                  'Rivaroxaban', 'Apixaban'])]\n",
    "\n",
    "\n",
    "im_Teders_df['Contr_SignDate'] = pd.to_datetime(im_Teders_df['Contr_SignDate'])\n",
    "im_Teders_df['Месяц_подписания_контракта'] = im_Teders_df['Contr_SignDate'].dt.month\n",
    "im_Teders_df['Год_подписания_контракта'] = im_Teders_df['Contr_SignDate'].dt.year\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df = im_Teders_df_copy[[\n",
    "       'Lot_LotStructure', 'Lot_LotType', 'Lot_LotNm', 'Lot_SupplyReglament',\n",
    "       'Lot_LotRegion', 'Lot_PlanName', 'Lot_PlanTVal', 'Lot_StorageLifeNM',\n",
    "       'Lot_StorageLifeText', 'Tender_FormT_name',\n",
    "       'Tender_ChannelFinanceNm', 'Tender_FO_Nm',\n",
    "        'Tender_Budgets_Name', 'Tender_TenderDocReglament',\n",
    "       'Tender_TendNm',  'Tender_NSI_Law',\n",
    "       'Prod_Producer', 'Prod_Form',\n",
    "       'Contr_BudgetChannelNm', 'Contr_Su_OrganizationName',\n",
    "       'Contr_SingleCustomerReason',\n",
    "       'Customer_OrgNmS',  'Customer_RegNm',\n",
    "       'Customer_OrgTypeName',\n",
    "       'Customer_ShottypeLPY', 'Customer_typeLPY','LotSpec_MNN_InnE']]\n",
    "\n",
    "im_Teders_df['data'] = df.apply(lambda x: '*'.join(x.dropna().astype(str).values), axis=1)\n",
    "im_Teders_df['data'] = im_Teders_df['data'].str.replace(\"\\r\", \" \")\n",
    "im_Teders_df['data'] = im_Teders_df['data'].str.replace(\"\\n\", \" \")\n",
    "im_Teders_df['data'] = im_Teders_df['data'].str.replace(\"    \", \" \")\n",
    "im_Teders_df['data'] = im_Teders_df['data'].str.replace('\"', '')\n",
    "im_Teders_df['data'] = im_Teders_df['data'].str.lower()\n",
    "\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "\n",
    "for punct_sign in punctuation_signs:\n",
    "    im_Teders_df['data'] = im_Teders_df['data'].str.replace(punct_sign, '')\n",
    "\n",
    "\n",
    "im_Teders_df['data'] = im_Teders_df['data'].apply(preprocess_text)\n",
    "category_codes = {\n",
    "    'Hospital': 0,\n",
    "    'COVID': 1,\n",
    "    'Retail': 2,\n",
    "    'Reimbursement': 3,\n",
    "    'Program': 4\n",
    "}\n",
    "\n",
    "im_Teders_df['mark_Code'] = im_Teders_df[im_Teders_df['Mark'] != 'Service']['Mark']\n",
    "im_Teders_df = im_Teders_df.replace({'mark_Code':category_codes})\n",
    "\n",
    "df_with_mark_code = im_Teders_df[im_Teders_df['mark_Code'].notnull()]\n",
    "df_for_predict = im_Teders_df\n",
    "df_for_predict['mark_Code'] = ''\n",
    "im_Teders_df['Mark'] = im_Teders_df[im_Teders_df['Mark'] != 'Service']['Mark']\n",
    "df_for_training = df_with_mark_code[(df_with_mark_code['Contr_SignDate']>='2020-06-01')]\n",
    "df_for_training.groupby(df_for_training['mark_Code']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_for_training['data'],\n",
    "                                                    df_for_training['mark_Code'],\n",
    "                                                    test_size=.15,\n",
    "                                                    random_state=8)\n",
    "X = df_for_predict['data']\n",
    "\n",
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300\n",
    "\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "\n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoosting\n",
    "\n",
    "n_estimators = [200, 800]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [10, 40]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [10, 30, 50]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "learning_rate = [.1, .5]\n",
    "subsample = [.5, 1.]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'learning_rate': learning_rate,\n",
    "               'subsample': subsample}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the base model to tune\n",
    "gbc = GradientBoostingClassifier(random_state=8)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=gbc,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3,\n",
    "                                   verbose=1,\n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(features_train, labels_train)\n",
    "\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search\n",
    "max_depth = [10]\n",
    "max_features = ['sqrt']\n",
    "min_samples_leaf = [2]\n",
    "min_samples_split = [50]\n",
    "n_estimators = [800]\n",
    "learning_rate = [.1]\n",
    "subsample = [1.]\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': max_depth,\n",
    "    'max_features': max_features,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'n_estimators': n_estimators,\n",
    "    'learning_rate': learning_rate,\n",
    "    'subsample': subsample\n",
    "\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "gbc = GradientBoostingClassifier(random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=gbc,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbc = grid_search.best_estimator_\n",
    "best_gbc.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_pred = best_gbc.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(labels_train, best_gbc.predict(features_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(labels_test, gbc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification report\")\n",
    "print(classification_report(labels_test,gbc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "aux_df = df_for_training[['Mark', 'mark_Code']].drop_duplicates().sort_values('mark_Code')\n",
    "conf_matrix = confusion_matrix(labels_test, gbc_pred)\n",
    "plt.figure(figsize=(12.8,6))\n",
    "sns.heatmap(conf_matrix,\n",
    "            annot=True,\n",
    "            xticklabels=aux_df['Mark'].values,\n",
    "            yticklabels=aux_df['Mark'].values,\n",
    "            cmap=\"Blues\",\n",
    "            fmt='g')\n",
    "\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = GradientBoostingClassifier(random_state = 8)\n",
    "base_model.fit(features_train, labels_train)\n",
    "accuracy_score(labels_test, base_model.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbc.fit(features_train, labels_train)\n",
    "accuracy_score(labels_test, best_gbc.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((features_train,features_test), axis=0)\n",
    "labels = np.concatenate((labels_train,labels_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dim_red(\"TSNE\",\n",
    "             features=features,\n",
    "             labels=labels,\n",
    "             n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf.transform(X).toarray()\n",
    "gbc_pred = best_gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GradientBoostingClassifier')\n",
    "plot_dim_red(\"TSNE\",\n",
    "             features=X_test,\n",
    "             labels=gbc_pred,\n",
    "             n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_predict['mark_Code'] = gbc_pred\n",
    "im_Teders_df = df_for_predict\n",
    "codes_category = {\n",
    "    0:'Hospital',\n",
    "    1:'COVID',\n",
    "    2:'Retail',\n",
    "    3:'Reimbursement',\n",
    "    4:'Program'\n",
    "}\n",
    "\n",
    "im_Teders_df = im_Teders_df.replace({'mark_Code':codes_category})\n",
    "im_Teders_df.loc[((im_Teders_df['mark_Code'].isin(['Program','Reimbursement','COVID']))&\\\n",
    "                 (im_Teders_df['mark_Code'] != 'Варфарин')), 'DoT_Price'] = 52.2\n",
    "im_Teders_df.loc[((im_Teders_df['mark_Code'].isin(['Hospital','Retail']))&\\\n",
    "                 (im_Teders_df['mark_Code'] != 'Варфарин')), 'DoT_Price'] = 68.7\n",
    "im_Teders_df['Summa_with_sale_PRC'] = im_Teders_df['DoT_Price']*im_Teders_df['Prod_DOT']\n",
    "im_tenders_allmarks_df = im_Teders_df \n",
    "\n",
    "# Write recipe outputs\n",
    "im_tenders_allmarks = dataiku.Dataset(\"IM_tenders_allmarks\")\n",
    "im_tenders_allmarks.write_with_schema(im_tenders_allmarks_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
